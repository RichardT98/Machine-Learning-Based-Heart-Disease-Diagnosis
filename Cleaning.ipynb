{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stone-decimal",
   "metadata": {},
   "source": [
    "The initial, originally published Cleveland dataset \"cleveland.data\" contains 76 attributes, but all published literature refer to using a subset of 14 of them, which is available in the UCI Machine Learning Repository as \"processed.cleveland.data\".\n",
    "\n",
    "Beside the 14 features that most of the researchers are considering, many of the attributes in the dataset have missing values or errors, that are possibly affecting the performances of the models. Furthermore, the other attributes beside these 14 are not directly related to heart diseases or may not have a strong correlation with the target variable, because they are just specifications of some of the 14 variables. Analyzing too many attributes can result in overly complex models that are prone to overfitting. It is possible to simplify the models and enhance their generalization abilities by employing this selection of features. That is why my research is also focusing on this subset of the Cleveland dataset.\n",
    "\n",
    "It is important to note, that we will refer to the processed version of the Cleveland data (which has 14 columns) in the following as \"cleveland.data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "promising-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the \"cleveland.data\" to CSV file format:\n",
    "def data_to_csv(data_file_path, csv_file_path):\n",
    "    with open(data_file_path, 'r') as data_file:\n",
    "        data = data_file.readlines()\n",
    "\n",
    "    with open(csv_file_path, 'w') as csv_file:\n",
    "        for line in data:\n",
    "            csv_file.write(','.join(line.strip().split()))\n",
    "            csv_file.write('\\n')\n",
    "\n",
    "data_to_csv('cleveland.data', 'clevelanddata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "specified-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the \"Statlog.data\" to CSV file format:\n",
    "def data_to_csv(data_file_path, csv_file_path):\n",
    "    with open(data_file_path, 'r') as data_file:\n",
    "        data = data_file.readlines()\n",
    "\n",
    "    with open(csv_file_path, 'w') as csv_file:\n",
    "        for line in data:\n",
    "            csv_file.write(','.join(line.strip().split()))\n",
    "            csv_file.write('\\n')\n",
    "\n",
    "data_to_csv('Statlog.dat', 'statlogdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "common-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'absence or presence of heart disease']\n",
      "['63.0', '1.0', '1.0', '145.0', '233.0', '1.0', '2.0', '150.0', '0.0', '2.3', '3.0', '0.0', '6.0', '0']\n",
      "['67.0', '1.0', '4.0', '160.0', '286.0', '0.0', '2.0', '108.0', '1.0', '1.5', '2.0', '3.0', '3.0', '2']\n",
      "['67.0', '1.0', '4.0', '120.0', '229.0', '0.0', '2.0', '129.0', '1.0', '2.6', '2.0', '2.0', '7.0', '1']\n",
      "['37.0', '1.0', '3.0', '130.0', '250.0', '0.0', '0.0', '187.0', '0.0', '3.5', '3.0', '0.0', '3.0', '0']\n"
     ]
    }
   ],
   "source": [
    "# This code creates the header for the cleveland dataset with the descriprions from the UCI Website\n",
    "\n",
    "import csv\n",
    "\n",
    "# Defining the header according to the UCI Website\n",
    "header = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "          'exang', 'oldpeak', 'slope', 'ca', 'thal', 'absence or presence of heart disease']\n",
    "\n",
    "# Open the CSV for reading\n",
    "with open('clevelanddata.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "    # Open new CSV file for writing\n",
    "    with open('clevelanddraft.csv', 'w', newline='') as new_csv_file:\n",
    "        writer = csv.writer(new_csv_file)\n",
    "\n",
    "        # Create the header\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Copy the rows\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "        \n",
    "# CHECKING THE RESULTS\n",
    "# Open the final CSV file\n",
    "with open('clevelanddraft.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "    # Print the first 5 rows incl. header\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 5:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "extra-switzerland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'absence or presence of heart disease']\n",
      "['70.0', '1.0', '4.0', '130.0', '322.0', '0.0', '2.0', '109.0', '0.0', '2.4', '2.0', '3.0', '3.0', '2']\n",
      "['67.0', '0.0', '3.0', '115.0', '564.0', '0.0', '2.0', '160.0', '0.0', '1.6', '2.0', '0.0', '7.0', '1']\n",
      "['57.0', '1.0', '2.0', '124.0', '261.0', '0.0', '0.0', '141.0', '0.0', '0.3', '1.0', '0.0', '7.0', '2']\n",
      "['64.0', '1.0', '4.0', '128.0', '263.0', '0.0', '0.0', '105.0', '1.0', '0.2', '2.0', '1.0', '7.0', '1']\n"
     ]
    }
   ],
   "source": [
    "# This code now creates the header for the statlog dataset with descriprions from the UCI Website\n",
    "\n",
    "# Defining the header according to the UCI Website\n",
    "header = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "          'exang', 'oldpeak', 'slope', 'ca', 'thal', 'absence or presence of heart disease']\n",
    "\n",
    "# Open the CSV for reading\n",
    "with open('statlogdata.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "    # Open new CSV file for writing\n",
    "    with open('statlogdraft.csv', 'w', newline='') as new_csv_file:\n",
    "        writer = csv.writer(new_csv_file)\n",
    "\n",
    "        # Create the header to new CSV\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Copy the rows from the original CSV file\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            \n",
    "# CHECKING THE RESULTS\n",
    "# Open the final CSV file\n",
    "with open('statlogdraft.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "    # Print the first 5 rows incl. header\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 5:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dangerous-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: this code transforms the values of the last column of the Cleveland dataset, because originally it is: 0 = absence of heart disease, 1,2,3,4 = presence of heart disease. For easier handling, I transformed 0 to 1, and every value > 0 to 2.\n",
    "\n",
    "# iloc is used to select all rows and the last column, the applied method is used to apply a lambda function to every value in the last column, which transforms 0 to 1 and any value greater than 0 to 2\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('clevelanddraft.csv')\n",
    "\n",
    "# Transforming the last column\n",
    "data.iloc[:, -1] = data.iloc[:, -1].apply(lambda x: 1 if x == 0 else 2 if x > 0 else x)\n",
    "\n",
    "# Write the transformed data to a new file called 'cleveland.csv'\n",
    "data.to_csv('clevelanddraft2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "excess-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replacing the remaining missing values and incorrect entries in both datasets with NaN\n",
    "\n",
    "# For the Cleveland data:\n",
    "\n",
    "df = pd.read_csv('clevelanddraft2.csv')\n",
    "\n",
    "# Replace ?s and missing values with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Save the updated dataframe back to a csv file\n",
    "df.to_csv('cleveland.csv', index=False)\n",
    "\n",
    "# For the Statlog data:\n",
    "\n",
    "df = pd.read_csv('statlogdraft.csv')\n",
    "\n",
    "# Replace ?s and missing values with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Save the updated dataframe back to a csv file\n",
    "df.to_csv('statlog.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-outdoors",
   "metadata": {},
   "source": [
    "Both Datasets had some missing values or entries \"?\". Both are replaced by NaNs.\n",
    "\n",
    "Using the datasets with NaNs will cause errors while performing ML classifications. Taking the mean of a given column to replace the NaNs can be a problematic approach because it assumes that the NaN values are missing completely at random (MCAR) and that the mean of the other data in the column are representative of these. This assumption can lead to biased or unreliable results, as this is a relatively small dataset and represents real symptoms and health data of individuals.\n",
    "\n",
    "Therefore, we are removing all rows with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "proved-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all rows in both datasets containing NaNs\n",
    "clevelanddata = pd.read_csv('cleveland.csv') \n",
    "statlogdata = pd.read_csv('statlog.csv') \n",
    "\n",
    "clevelanddata.dropna(inplace=True)\n",
    "statlogdata.dropna(inplace=True)\n",
    "\n",
    "clevelanddata.to_csv('cleveland_cleaned.csv', index=False)\n",
    "statlogdata.to_csv('statlog_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "starting-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 297 entries, 0 to 296\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   age                                   297 non-null    float64\n",
      " 1   sex                                   297 non-null    float64\n",
      " 2   cp                                    297 non-null    float64\n",
      " 3   trestbps                              297 non-null    float64\n",
      " 4   chol                                  297 non-null    float64\n",
      " 5   fbs                                   297 non-null    float64\n",
      " 6   restecg                               297 non-null    float64\n",
      " 7   thalach                               297 non-null    float64\n",
      " 8   exang                                 297 non-null    float64\n",
      " 9   oldpeak                               297 non-null    float64\n",
      " 10  slope                                 297 non-null    float64\n",
      " 11  ca                                    297 non-null    float64\n",
      " 12  thal                                  297 non-null    float64\n",
      " 13  absence or presence of heart disease  297 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 32.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of the cleaned Cleveland dataset\n",
    "cleveland = pd.read_csv('cleveland_cleaned.csv') \n",
    "cleveland.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excellent-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 270 entries, 0 to 269\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   age                                   270 non-null    float64\n",
      " 1   sex                                   270 non-null    float64\n",
      " 2   cp                                    270 non-null    float64\n",
      " 3   trestbps                              270 non-null    float64\n",
      " 4   chol                                  270 non-null    float64\n",
      " 5   fbs                                   270 non-null    float64\n",
      " 6   restecg                               270 non-null    float64\n",
      " 7   thalach                               270 non-null    float64\n",
      " 8   exang                                 270 non-null    float64\n",
      " 9   oldpeak                               270 non-null    float64\n",
      " 10  slope                                 270 non-null    float64\n",
      " 11  ca                                    270 non-null    float64\n",
      " 12  thal                                  270 non-null    float64\n",
      " 13  absence or presence of heart disease  270 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 29.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of the cleaned Statlog dataset\n",
    "statlog = pd.read_csv('statlog_cleaned.csv')\n",
    "statlog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "opposed-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later experimentation, we combine the two datasets as a new CSV:\n",
    "\n",
    "cleveland_df = pd.read_csv('cleveland_cleaned.csv')\n",
    "statlog_df = pd.read_csv('statlog_cleaned.csv')\n",
    "\n",
    "# merge the two dataframes\n",
    "combined_df = pd.concat([cleveland_df, statlog_df], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "# shuffle the rows randomly, so the entries will be mixed\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# save the dataframe to a CSV\n",
    "combined_df.to_csv('combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sonic-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 567 entries, 0 to 566\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   age                                   567 non-null    float64\n",
      " 1   sex                                   567 non-null    float64\n",
      " 2   cp                                    567 non-null    float64\n",
      " 3   trestbps                              567 non-null    float64\n",
      " 4   chol                                  567 non-null    float64\n",
      " 5   fbs                                   567 non-null    float64\n",
      " 6   restecg                               567 non-null    float64\n",
      " 7   thalach                               567 non-null    float64\n",
      " 8   exang                                 567 non-null    float64\n",
      " 9   oldpeak                               567 non-null    float64\n",
      " 10  slope                                 567 non-null    float64\n",
      " 11  ca                                    567 non-null    float64\n",
      " 12  thal                                  567 non-null    float64\n",
      " 13  absence or presence of heart disease  567 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 62.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of the Combined dataset\n",
    "combineddataset = pd.read_csv('combined.csv')\n",
    "combineddataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-marker",
   "metadata": {},
   "source": [
    "Description of the Attributes used in both datasets:\n",
    "\n",
    "\n",
    "age: patient age in number\n",
    "\n",
    "sex: 1.0=male, 0.0=female\n",
    "\n",
    "cp: chest pain type (Value 1.0: typical angina, Value 2.0: atypical angina, Value 3.0: non-anginal pain, Value 4.0: asymptomatic)\n",
    "\n",
    "trestbp: resting blood pressure in Hgmm\n",
    "\n",
    "chol: serum cholestoral in mg/dl\n",
    "\n",
    "fbs: fasting blood sugar > 120 mg/dl (1.0 = true, 0.0 = false)\n",
    "\n",
    "restecg: resting electrocardiographic results (Value 0.0: normal, Value 1.0: having ST-T wave abnormality, Value 2.0: showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "\n",
    "thalach: maximum heart rate achieved\n",
    "\n",
    "exang: exercise induced angina (1.0 = yes, 0.0 = no)\n",
    "\n",
    "oldpeak = ST depression induced by exercise relative to rest\n",
    "\n",
    "slope: the slope of the peak exercise ST segment (Value 1.0: upsloping, Value 2.0: flat, Value 3.0: downsloping)\n",
    "\n",
    "ca: number of major vessels (0.0 - 3.0) colored by flourosopy\n",
    "\n",
    "thal: displays the thalassemia (3.0 = normal, 6.0 = fixed defect, 7.0 = reversable defect)\n",
    "\n",
    "absence or presence of heart disease: 1=absence, 2=presence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
